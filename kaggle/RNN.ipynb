{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "select patient_id, dispense_year, IF(SUM(IF(CAT_chronic_illness = \"Diabetes\",1,0))>0,1,0) as diab, MAX(pat_regional_category) as regional_cat, SUM(1) as trans_cnt\n",
    "from dthon.transactions_enriched\n",
    "where patient_id <= 279200\n",
    "AND patient_id >= 558353\n",
    "AND dispense_year < 2017\n",
    "group by patient_id, dispense_year "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do (basic):\n",
    "- divide by patient_id into prediction set for 2016 (kaggle) and the rest into train + test (done)\n",
    "- identify automatically number of features (done)\n",
    "- train the model (done)\n",
    "- send prediction to kaggle (done)\n",
    "\n",
    "Advanced:\n",
    "- save a model\n",
    "- add batch normalization (done)\n",
    "- run on ML engine\n",
    "- set pipeline to load from google storage into model\n",
    "\n",
    "Additional model:\n",
    "- RNN - data sorted by patient and  years, chunked into patients and padded, so that everytime there is 2008 - 2015\n",
    "\n",
    "Tensorflow 1.1 + Python 3.5/3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.1 |Continuum Analytics, Inc.| (default, Mar 22 2017, 19:54:23) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import google.datalab.bigquery as bq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "project_id = \"the-d-team-164512\"\n",
    "table_name = 'ml_model'\n",
    "scheme = bq.Table('dthon.'+table_name).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pivot =''\n",
    "for i in (np.arange(2, len(scheme))):\n",
    "    for year in (np.arange(2008, 2017)):\n",
    "        field = str(scheme[i]['name'])\n",
    "        pivot = pivot + \",SUM(IF(dispense_year = \" + str(year) + \", \" + field + \", 0)) as \" + field + \"_\" + str(year) + \"\"\"\n",
    "\"\"\"\n",
    "query = \"\"\"SELECT patient_id \n",
    "\"\"\" + pivot + \"FROM dthon.\" + table_name + \"\"\"\n",
    "GROUP BY patient_id\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT patient_id \n",
      ",SUM(IF(dispense_year = 2008, diab, 0)) as diab_2008\n",
      ",SUM(IF(dispense_year = 2009, diab, 0)) as diab_2009\n",
      ",SUM(IF(dispense_year = 2010, diab, 0)) as diab_2010\n",
      ",SUM(IF(dispense_year = 2011, diab, 0)) as diab_2011\n",
      ",SUM(IF(dispense_year = 2012, diab, 0)) as diab_2012\n",
      ",SUM(IF(dispense_year = 2013, diab, 0)) as diab_2013\n",
      ",SUM(IF(dispense_year = 2014, diab, 0)) as diab_2014\n",
      ",SUM(IF(dispense_year = 2015, diab, 0)) as diab_2015\n",
      ",SUM(IF(dispense_year = 2016, diab, 0)) as diab_2016\n",
      ",SUM(IF(dispense_year = 2008, regional_cat, 0)) as regional_cat_2008\n",
      ",SUM(IF(dispense_year = 2009, regional_cat, 0)) as regional_cat_2009\n",
      ",SUM(IF(dispense_year = 2010, regional_cat, 0)) as regional_cat_2010\n",
      ",SUM(IF(dispense_year = 2011, regional_cat, 0)) as regional_cat_2011\n",
      ",SUM(IF(dispense_year = 2012, regional_cat, 0)) as regional_cat_2012\n",
      ",SUM(IF(dispense_year = 2013, regional_cat, 0)) as regional_cat_2013\n",
      ",SUM(IF(dispense_year = 2014, regional_cat, 0)) as regional_cat_2014\n",
      ",SUM(IF(dispense_year = 2015, regional_cat, 0)) as regional_cat_2015\n",
      ",SUM(IF(dispense_year = 2016, regional_cat, 0)) as regional_cat_2016\n",
      ",SUM(IF(dispense_year = 2008, trans_cnt, 0)) as trans_cnt_2008\n",
      ",SUM(IF(dispense_year = 2009, trans_cnt, 0)) as trans_cnt_2009\n",
      ",SUM(IF(dispense_year = 2010, trans_cnt, 0)) as trans_cnt_2010\n",
      ",SUM(IF(dispense_year = 2011, trans_cnt, 0)) as trans_cnt_2011\n",
      ",SUM(IF(dispense_year = 2012, trans_cnt, 0)) as trans_cnt_2012\n",
      ",SUM(IF(dispense_year = 2013, trans_cnt, 0)) as trans_cnt_2013\n",
      ",SUM(IF(dispense_year = 2014, trans_cnt, 0)) as trans_cnt_2014\n",
      ",SUM(IF(dispense_year = 2015, trans_cnt, 0)) as trans_cnt_2015\n",
      ",SUM(IF(dispense_year = 2016, trans_cnt, 0)) as trans_cnt_2016\n",
      "FROM dthon.ml_model\n",
      "GROUP BY patient_id\n"
     ]
    }
   ],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting query... ok.\n",
      "Query running...\n",
      "Query done.\n",
      "Cache hit.\n",
      "\n",
      "Retrieving results...\n",
      "  Got page: 1; 2% done. Elapsed 7.48 s.\n",
      "  Got page: 2; 4% done. Elapsed 12.33 s.\n",
      "  Got page: 3; 7% done. Elapsed 18.26 s.\n",
      "  Got page: 4; 9% done. Elapsed 23.73 s.\n",
      "  Got page: 5; 11% done. Elapsed 29.57 s.\n",
      "  Got page: 6; 13% done. Elapsed 34.86 s.\n",
      "  Got page: 7; 16% done. Elapsed 40.44 s.\n",
      "  Got page: 8; 18% done. Elapsed 45.85 s.\n",
      "  Got page: 9; 20% done. Elapsed 51.72 s.\n",
      "  Got page: 10; 22% done. Elapsed 57.6 s.\n",
      "  Got page: 11; 25% done. Elapsed 63.72 s.\n",
      "  Got page: 12; 27% done. Elapsed 70.3 s.\n",
      "  Got page: 13; 29% done. Elapsed 75.88 s.\n",
      "  Got page: 14; 31% done. Elapsed 81.61 s.\n",
      "  Got page: 15; 34% done. Elapsed 87.07 s.\n",
      "  Got page: 16; 36% done. Elapsed 92.68 s.\n",
      "  Got page: 17; 38% done. Elapsed 98.17 s.\n",
      "  Got page: 18; 40% done. Elapsed 103.78 s.\n",
      "  Got page: 19; 43% done. Elapsed 109.45 s.\n",
      "  Got page: 20; 45% done. Elapsed 115.64 s.\n",
      "  Got page: 21; 47% done. Elapsed 120.6 s.\n",
      "  Got page: 22; 49% done. Elapsed 126.98 s.\n",
      "  Got page: 23; 51% done. Elapsed 134.34 s.\n",
      "  Got page: 24; 54% done. Elapsed 140.2 s.\n",
      "  Got page: 25; 56% done. Elapsed 145.66 s.\n",
      "  Got page: 26; 58% done. Elapsed 151.01 s.\n",
      "  Got page: 27; 60% done. Elapsed 156.18 s.\n",
      "  Got page: 28; 63% done. Elapsed 162.12 s.\n",
      "  Got page: 29; 65% done. Elapsed 168.29 s.\n",
      "  Got page: 30; 67% done. Elapsed 174.01 s.\n",
      "  Got page: 31; 69% done. Elapsed 180.23 s.\n",
      "  Got page: 32; 72% done. Elapsed 185.65 s.\n",
      "  Got page: 33; 74% done. Elapsed 191.96 s.\n",
      "  Got page: 34; 76% done. Elapsed 198.17 s.\n",
      "  Got page: 35; 78% done. Elapsed 203.64 s.\n",
      "  Got page: 36; 81% done. Elapsed 208.92 s.\n",
      "  Got page: 37; 83% done. Elapsed 215.87 s.\n",
      "  Got page: 38; 85% done. Elapsed 222.26 s.\n",
      "  Got page: 39; 87% done. Elapsed 228.24 s.\n",
      "  Got page: 40; 90% done. Elapsed 234.44 s.\n",
      "  Got page: 41; 92% done. Elapsed 239.77 s.\n",
      "  Got page: 42; 94% done. Elapsed 245.38 s.\n",
      "  Got page: 43; 96% done. Elapsed 250.85 s.\n",
      "  Got page: 44; 99% done. Elapsed 256.72 s.\n",
      "  Got page: 45; 100% done. Elapsed 261.23 s.\n",
      "Got 558352 rows.\n",
      "\n",
      "Total time taken 287.83 s.\n",
      "Finished at 2017-05-26 13:47:53.\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_gbq(query, project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_set = dataset.loc[(dataset['patient_id'] > 279200) & (dataset['patient_id'] < 558353), :]\n",
    "model_set = dataset.loc[(dataset['patient_id'] <= 279200) | (dataset['patient_id'] >= 558353), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunk_size = len(scheme) - 2\n",
    "chunks_no = len(np.arange(2008, 2017))\n",
    "labels_no = len(np.unique(dataset['diab_2016']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "9\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(labels_no)\n",
    "print(chunks_no)\n",
    "print(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = model_set.drop(['patient_id'], axis=1)\n",
    "features['diab_2016'] = 0\n",
    "labels = model_set['diab_2016']\n",
    "features_pred = predict_set.drop(['patient_id'], axis=1)\n",
    "features_pred['diab_2016'] = 0\n",
    "labels_pred = predict_set[['patient_id','diab_2016']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logs_path = './logs/tf_model'\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = 0.2)\n",
    "\n",
    "batch_size =1000\n",
    "nodes_1 = 500\n",
    "nodes_2 = 300\n",
    "\n",
    "def init_weights(nodes):\n",
    "    return 2 / np.sqrt(nodes)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    inp = tf.placeholder(tf.float32, shape=[None, chunk_size*chunks_no])\n",
    "    y = tf.placeholder(tf.int32)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    labels = tf.one_hot(y, depth=2)\n",
    "    \n",
    "    with tf.name_scope('parameters'):\n",
    "        #hidden_1 = {\n",
    "        #    'weights': tf.Variable(tf.truncated_normal([features_no, nodes_1], stddev=init_weights(features_no))),\n",
    "        #    'biases': tf.Variable(tf.zeros([nodes_1]))}\n",
    "        hidden_2 = {'weights': tf.Variable(tf.truncated_normal([nodes_1, nodes_2], stddev=init_weights(nodes_1))),\n",
    "                   'biases': tf.Variable(tf.zeros([nodes_2]))}\n",
    "        output = {'weights': tf.Variable(tf.truncated_normal([nodes_2, labels_no], stddev=init_weights(nodes_2))),\n",
    "                  'biases': tf.Variable(tf.zeros([labels_no]))}\n",
    "    with tf.name_scope('model'):\n",
    "        inp2 = tf.reshape(inp, [-1, chunk_size, chunks_no])\n",
    "        inp2 = tf.transpose(inp2, perm =[0, 2, 1])\n",
    "        lstm_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(num_units = 64, dropout_keep_prob = keep_prob)\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell=lstm_cell, inputs=inp2, sequence_length=chunks_no)\n",
    "        layer_1 = outputs[:,:,(chunks_no-2):chunks_no]\n",
    "        layer_2 = tf.matmul(layer_1, hidden_2['weights']) + hidden_2['biases']\n",
    "        layer_2 = tf.contrib.layers.batch_norm(inputs=layer_2, is_training=is_training, updates_collections=None)\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "        layer_2 = tf.nn.dropout(layer_2, keep_prob=keep_prob)\n",
    "        logits = tf.matmul(layer_2, output['weights']) + output['biases']\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.name_scope('loss'):\n",
    "        cost = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "    with tf.name_scope('optimization'):\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    with tf.name_scope('metrics'):\n",
    "        #auc, _ = tf.metrics.auc(labels=labels, predictions=prediction)\n",
    "        accuracy= tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits,1),tf.argmax(labels, 1)), tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    #tf.summary.scalar('auc', auc)\n",
    "    merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training accuracy: 0.972222 Test accuracy: 0.938770408163\n",
      "Epoch: 1 Training accuracy: 0.952778 Test accuracy: 0.950532312925\n",
      "Epoch: 2 Training accuracy: 0.961111 Test accuracy: 0.960227040816\n",
      "Epoch: 3 Training accuracy: 0.961111 Test accuracy: 0.961351190476\n",
      "Epoch: 4 Training accuracy: 0.969444 Test accuracy: 0.964596938776\n",
      "Epoch: 5 Training accuracy: 0.969444 Test accuracy: 0.964965136054\n",
      "Epoch: 6 Training accuracy: 0.95 Test accuracy: 0.965307823129\n",
      "Epoch: 7 Training accuracy: 0.958333 Test accuracy: 0.965825680272\n",
      "Epoch: 8 Training accuracy: 0.972222 Test accuracy: 0.965039965986\n",
      "Epoch: 9 Training accuracy: 0.977778 Test accuracy: 0.96412585034\n",
      "Epoch: 10 Training accuracy: 0.975 Test accuracy: 0.965072278912\n",
      "Epoch: 11 Training accuracy: 0.977778 Test accuracy: 0.966054421769\n",
      "Epoch: 12 Training accuracy: 0.963889 Test accuracy: 0.966090136054\n",
      "Epoch: 13 Training accuracy: 0.955556 Test accuracy: 0.96575085034\n",
      "Epoch: 14 Training accuracy: 0.969444 Test accuracy: 0.966054421769\n",
      "Epoch: 15 Training accuracy: 0.975 Test accuracy: 0.966086734694\n",
      "Epoch: 16 Training accuracy: 0.969444 Test accuracy: 0.965840136054\n",
      "Epoch: 17 Training accuracy: 0.966667 Test accuracy: 0.965947278912\n",
      "Epoch: 18 Training accuracy: 0.958333 Test accuracy: 0.96550085034\n",
      "Epoch: 19 Training accuracy: 0.972222 Test accuracy: 0.965890306122\n",
      "Epoch: 20 Training accuracy: 0.969444 Test accuracy: 0.965897108844\n",
      "Epoch: 21 Training accuracy: 0.969444 Test accuracy: 0.965943877551\n",
      "Epoch: 22 Training accuracy: 0.955556 Test accuracy: 0.966057823129\n",
      "Epoch: 23 Training accuracy: 0.972222 Test accuracy: 0.966079931973\n",
      "Epoch: 24 Training accuracy: 0.975 Test accuracy: 0.965693877551\n",
      "Epoch: 25 Training accuracy: 0.975 Test accuracy: 0.96606547619\n",
      "Epoch: 26 Training accuracy: 0.975 Test accuracy: 0.966033163265\n",
      "Epoch: 27 Training accuracy: 0.95 Test accuracy: 0.966015306122\n",
      "Epoch: 28 Training accuracy: 0.986111 Test accuracy: 0.966172619048\n",
      "Epoch: 29 Training accuracy: 0.95 Test accuracy: 0.965965136054\n",
      "Epoch: 30 Training accuracy: 0.972222 Test accuracy: 0.966090136054\n",
      "Epoch: 31 Training accuracy: 0.955556 Test accuracy: 0.965765306122\n",
      "Epoch: 32 Training accuracy: 0.972222 Test accuracy: 0.965322278912\n",
      "Epoch: 33 Training accuracy: 0.966667 Test accuracy: 0.965768707483\n",
      "Epoch: 34 Training accuracy: 0.958333 Test accuracy: 0.966268707483\n",
      "Epoch: 35 Training accuracy: 0.952778 Test accuracy: 0.965836734694\n",
      "Epoch: 36 Training accuracy: 0.972222 Test accuracy: 0.96631547619\n",
      "Epoch: 37 Training accuracy: 0.969444 Test accuracy: 0.965840136054\n",
      "Epoch: 38 Training accuracy: 0.966667 Test accuracy: 0.965857993197\n",
      "Epoch: 39 Training accuracy: 0.963889 Test accuracy: 0.96587585034\n",
      "Epoch: 40 Training accuracy: 0.947222 Test accuracy: 0.966176020408\n",
      "Epoch: 41 Training accuracy: 0.963889 Test accuracy: 0.965482993197\n",
      "Epoch: 42 Training accuracy: 0.955556 Test accuracy: 0.965822278912\n",
      "Epoch: 43 Training accuracy: 0.969444 Test accuracy: 0.965851190476\n",
      "Epoch: 44 Training accuracy: 0.972222 Test accuracy: 0.965729591837\n",
      "Epoch: 45 Training accuracy: 0.955556 Test accuracy: 0.965943877551\n",
      "Epoch: 46 Training accuracy: 0.966667 Test accuracy: 0.966340136054\n",
      "Epoch: 47 Training accuracy: 0.961111 Test accuracy: 0.966107993197\n",
      "Epoch: 48 Training accuracy: 0.975 Test accuracy: 0.96594047619\n",
      "Epoch: 49 Training accuracy: 0.961111 Test accuracy: 0.966215136054\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    tf.local_variables_initializer().run()\n",
    "    epoch_no = 50\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=graph)\n",
    "    train_batches = int(np.ceil(features_train.shape[0]/batch_size))\n",
    "    test_batches = int(np.ceil(features_test.shape[0]/batch_size))\n",
    "    train_size = features_train.shape[0]\n",
    "    for epoch in np.arange(epoch_no):\n",
    "        order = np.random.permutation(train_size)\n",
    "        features_train = features_train.iloc[order,:]\n",
    "        labels_train = labels_train.iloc[order]\n",
    "        acc_t = []\n",
    "        for step in np.arange(train_batches):\n",
    "            _, summary, acc = sess.run([optimizer, merged_summary, accuracy], feed_dict = {\n",
    "                inp:features_train.iloc[(step*batch_size):((step+1)*batch_size),:],\n",
    "                y: labels_train.iloc[(step*batch_size):((step+1)*batch_size)],\n",
    "                keep_prob: 0.7,\n",
    "                is_training: True\n",
    "            })\n",
    "            summary_writer.add_summary(summary, epoch * train_batches + step)\n",
    "        for step in np.arange(test_batches):\n",
    "            predict = np.argmax(logits.eval(feed_dict = \n",
    "                                            {inp:features_test.iloc[(step*batch_size):((step+1)*batch_size),:],\n",
    "                                            keep_prob: 1.0,\n",
    "                                            is_training: False}), axis = 1)\n",
    "            acc_t.append(np.mean(predict == labels_test.iloc[(step*batch_size):((step+1)*batch_size)]))\n",
    "        print('Epoch: ' + str(epoch) + ' Training accuracy: ' + str(acc) + ' Test accuracy: ' + str(np.mean(acc_t)))\n",
    "    full_predict = list()\n",
    "    for step in np.arange(features_pred.shape[0]):\n",
    "        predict = prediction.eval(feed_dict = {inp:[features_pred.iloc[step,:]], \n",
    "                                               keep_prob: 1.0, \n",
    "                                               is_training: False})\n",
    "        full_predict.append(predict[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279152"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upload = pd.DataFrame({'patient_id': predict_set['patient_id'], 'prediction': full_predict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upload.to_csv('upload_file.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
